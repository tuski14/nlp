{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2hmQF8tawnc"
      },
      "source": [
        "___\n",
        "\n",
        "<a href='http://www.pieriandata.com'> <img src='../Pierian_Data_Logo.png' /></a>\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_mnYblGawng"
      },
      "source": [
        "# Sentence Segmentation\n",
        "In **spaCy Basics** we saw briefly how Doc objects are divided into sentences. In this section we'll learn how sentence segmentation works, and how to set our own segmentation rules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWt1Pm2zawnh"
      },
      "outputs": [],
      "source": [
        "# Perform standard imports\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SclZoUtGawni",
        "outputId": "97fbc5c0-6685-46ef-a6d4-87555d1bdce7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the first sentence.\n",
            "This is another sentence.\n",
            "This is the last sentence.\n"
          ]
        }
      ],
      "source": [
        "# From Spacy Basics:\n",
        "doc = nlp(u'This is the first sentence. This is another sentence. This is the last sentence.')\n",
        "\n",
        "for sent in doc.sents:\n",
        "    print(sent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PANndyDlawnj"
      },
      "source": [
        "### `Doc.sents` is a generator\n",
        "It is important to note that `doc.sents` is a *generator*. That is, a Doc is not segmented until `doc.sents` is called. This means that, where you could print the second Doc token with `print(doc[1])`, you can't call the \"second Doc sentence\" with `print(doc.sents[1])`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4WLZkZkawnj",
        "outputId": "5645c179-2814-48e7-9f75-95e91cc56801",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "is\n"
          ]
        }
      ],
      "source": [
        "print(doc[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwvWPKf0awnj",
        "outputId": "c2cdead2-3f9d-4fec-bb0e-930dea6b85bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'generator' object is not subscriptable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-2bc012eee1da>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'generator' object is not subscriptable"
          ]
        }
      ],
      "source": [
        "print(doc.sents[1])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(doc.sents)[1]\n"
      ],
      "metadata": {
        "id": "hmXnqyPKk3s9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57a6e9eb-7ff3-47e5-9bf4-d3c5dd3b6eee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "This is another sentence."
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZL0zHrZ_awnk"
      },
      "source": [
        "However, you *can* build a sentence collection by running `doc.sents` and saving the result to a list:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fffu2t-Aawnk",
        "outputId": "8f899104-2771-4cb4-ce5a-532e3b47ac53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[This is the first sentence.,\n",
              " This is another sentence.,\n",
              " This is the last sentence.]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "doc_sents = [sent for sent in doc.sents]\n",
        "doc_sents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWShmDjHawnl"
      },
      "source": [
        "<font color=green>**NOTE**: `list(doc.sents)` also works. We show a list comprehension as it allows you to pass in conditionals.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5QZvYg_awnl",
        "outputId": "a5b16086-aa2b-4fbb-8bd7-ea6ba14c0cb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This is another sentence.\n"
          ]
        }
      ],
      "source": [
        "# Now you can access individual sentences:\n",
        "print(doc_sents[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHyoYOrGawnl"
      },
      "source": [
        "### `sents` are Spans\n",
        "At first glance it looks like each `sent` contains text from the original Doc object. In fact they're just Spans with start and end token pointers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jxaj6Et8awnl",
        "outputId": "b8eeedb8-12a4-483f-8b93-c82f1ec71e6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.tokens.span.Span"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "type(doc_sents[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qMM_6tnawnm",
        "outputId": "b49a9f11-c9a1-4a56-9f88-40cded6f03ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6 11\n"
          ]
        }
      ],
      "source": [
        "print(doc_sents[1].start, doc_sents[1].end)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gl7rNZVlawnm"
      },
      "source": [
        "## Adding Rules\n",
        "spaCy's built-in `sentencizer` relies on the dependency parse and end-of-sentence punctuation to determine segmentation rules. We can add rules of our own, but they have to be added *before* the creation of the Doc object, as that is where the parsing of segment start tokens happens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45Bp3CNdawnm",
        "outputId": "c93c92d5-5af3-4e74-f219-4996571cc851",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True  This\n",
            "False  is\n",
            "False  a\n",
            "False  sentence\n",
            "False  .\n",
            "True  This\n",
            "False  is\n",
            "False  a\n",
            "False  sentence\n",
            "False  .\n",
            "True  This\n",
            "False  is\n",
            "False  a\n",
            "False  sentence\n",
            "False  .\n"
          ]
        }
      ],
      "source": [
        "# Parsing the segmentation start tokens happens during the nlp pipeline\n",
        "doc2 = nlp(u'This is a sentence. This is a sentence. This is a sentence.')\n",
        "\n",
        "for token in doc2:\n",
        "    print(token.is_sent_start, ' '+token.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBisDJVkawnm"
      },
      "source": [
        "<font color=green>Notice we haven't run `doc2.sents`, and yet `token.is_sent_start` was set to True on two tokens in the Doc.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AurKwciawnm"
      },
      "source": [
        "Let's add a semicolon to our existing segmentation rules. That is, whenever the sentencizer encounters a semicolon, the next token should start a new segment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snM-DDaTawnm",
        "outputId": "60859ec7-f519-4496-c0bb-d8d1b615a27b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Management is doing things right; leadership is doing the right things.\"\n",
            "-Peter Drucker\n"
          ]
        }
      ],
      "source": [
        "# SPACY'S DEFAULT BEHAVIOR\n",
        "doc3 = nlp(u'\"Management is doing things right; leadership is doing the right things.\" -Peter Drucker')\n",
        "\n",
        "for sent in doc3.sents:\n",
        "    print(sent)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_ygt2k6ZdAiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.pipe_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-5KW271dBOo",
        "outputId": "abde7d2a-b640-4fd5-a0bf-fbfa7a71d0c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.language import Language"
      ],
      "metadata": {
        "id": "cHiDW_rbgzGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBWEDauZawnn",
        "outputId": "133c74df-f9e9-4cee-ec9d-d418fece7054",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tok2vec',\n",
              " 'tagger',\n",
              " 'component',\n",
              " 'parser',\n",
              " 'attribute_ruler',\n",
              " 'lemmatizer',\n",
              " 'ner',\n",
              " 'entity_ruler']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# ADD A NEW RULE TO THE PIPELINE\n",
        "@Language.component(\"component\")\n",
        "def set_custom_boundaries(doc):\n",
        "    for token in doc[:-1]:\n",
        "        if token.text == ';':\n",
        "            doc[token.i+1].is_sent_start = True\n",
        "    return doc\n",
        "\n",
        "nlp.add_pipe(\"component\", before='parser')\n",
        "\n",
        "nlp.pipe_names"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9HQB0AYIfEFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrJLgJMEawnn"
      },
      "source": [
        "<font color=green>The new rule has to run before the document is parsed. Here we can either pass the argument `before='parser'` or `first=True`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRIh0TaEawnn",
        "outputId": "b67e5409-533d-4955-f182-7902ef98cc49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Management is doing things right;\n",
            "leadership is doing the right things.\"\n",
            "-Peter Drucker\n"
          ]
        }
      ],
      "source": [
        "# Re-run the Doc object creation:\n",
        "doc4 = nlp(u'\"Management is doing things right; leadership is doing the right things.\" -Peter Drucker')\n",
        "\n",
        "for sent in doc4.sents:\n",
        "    print(sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "te81rCFKawnn",
        "outputId": "fb42b014-4a40-4ac8-bd2d-aaa7d0f33d80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Management is doing things right; leadership is doing the right things.\"\n",
            "-Peter Drucker\n"
          ]
        }
      ],
      "source": [
        "# And yet the new rule doesn't apply to the older Doc object:\n",
        "for sent in doc3.sents:\n",
        "    print(sent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYqSv_gzawnn"
      },
      "source": [
        "### Why not change the token directly?\n",
        "Why not simply set the `.is_sent_start` value to True on existing tokens?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsjiS_g-awnn",
        "outputId": "c0eafeca-fb8b-47b6-9bb6-25775c1a0b62"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "leadership"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Find the token we want to change:\n",
        "doc3[7]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9Mh_s-pawno",
        "outputId": "bd2d0cc3-2b34-42f7-fa6c-e7cdedbce258"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "[E043] Refusing to write to token.sent_start if its document is parsed, because this may cause inconsistent state.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-5-bcec3fe6a9a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Try to change the .is_sent_start attribute:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdoc3\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_sent_start\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32mtoken.pyx\u001b[0m in \u001b[0;36mspacy.tokens.token.Token.is_sent_start.__set__\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: [E043] Refusing to write to token.sent_start if its document is parsed, because this may cause inconsistent state."
          ]
        }
      ],
      "source": [
        "# Try to change the .is_sent_start attribute:\n",
        "doc3[7].is_sent_start = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ks2RnCXawno"
      },
      "source": [
        "<font color=green>spaCy refuses to change the tag after the document is parsed to prevent inconsistencies in the data.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImvyHOzLawno"
      },
      "source": [
        "## Changing the Rules\n",
        "In some cases we want to *replace* spaCy's default sentencizer with our own set of rules. In this section we'll see how the default sentencizer breaks on periods. We'll then replace this behavior with a sentencizer that breaks on linebreaks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9D4qXV4awno",
        "outputId": "95111a89-61f4-4b49-9a40-74e89cc0453f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a sentence.\n",
            "This is another.\n",
            "\n",
            "\n",
            "This is a \n",
            "third sentence.\n"
          ]
        }
      ],
      "source": [
        "nlp = spacy.load('en_core_web_sm')  # reset to the original\n",
        "\n",
        "mystring = u\"This is a sentence. This is another.\\n\\nThis is a \\nthird sentence.\"\n",
        "\n",
        "# SPACY DEFAULT BEHAVIOR:\n",
        "doc = nlp(mystring)\n",
        "\n",
        "for sent in doc.sents:\n",
        "    print(sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUtWkGJ8awno",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "f94b7c72-49d9-4918-8569-6508a81c661e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "__init__() got an unexpected keyword argument 'strategy'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-dad3b452d003>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0msbd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentencizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit_on_newlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msbd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/pipeline/sentencizer.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.sentencizer.Sentencizer.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'strategy'"
          ]
        }
      ],
      "source": [
        "# CHANGING THE RULES\n",
        "from spacy.pipeline import SentenceSegmenter\n",
        "\n",
        "def split_on_newlines(doc):\n",
        "    start = 0\n",
        "    seen_newline = False\n",
        "    for word in doc:\n",
        "        if seen_newline:\n",
        "            yield doc[start:word.i]\n",
        "            start = word.i\n",
        "            seen_newline = False\n",
        "        elif word.text.startswith('\\n'): # handles multiple occurrences\n",
        "            seen_newline = True\n",
        "    yield doc[start:]      # handles the last group of tokens\n",
        "\n",
        "\n",
        "sbd = SentenceSegmenter(nlp.vocab, strategy=split_on_newlines)\n",
        "nlp.add_pipe(sbd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EcyFqe7awno"
      },
      "source": [
        "<font color=green>While the function `split_on_newlines` can be named anything we want, it's important to use the name `sbd` for the SentenceSegmenter.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGvsQxGPawno",
        "outputId": "8881148a-09a4-4794-ba22-8e0698f59f87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['This', 'is', 'a', 'sentence', '.', 'This', 'is', 'another', '.', '\\n\\n']\n",
            "['This', 'is', 'a', '\\n']\n",
            "['third', 'sentence', '.']\n"
          ]
        }
      ],
      "source": [
        "doc = nlp(mystring)\n",
        "for sent in doc.sents:\n",
        "    print([token.text for token in sent])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@Language.component(\"info_component\")\n",
        "def my_component(doc):\n",
        "    print(f\"After tokenization, this doc has {len(doc)} tokens.\")\n",
        "    print(\"The part-of-speech tags are:\", [token.pos_ for token in doc])\n",
        "    if len(doc) < 10:\n",
        "        print(\"This is a pretty short document.\")\n",
        "    return doc\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.add_pipe(\"info_component\", name=\"print_info\", last=True)\n",
        "print(nlp.pipe_names)  # ['tagger', 'parser', 'ner', 'print_info']\n",
        "doc = nlp(\"This is a sentence.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWv-0iZ5lamD",
        "outputId": "4c2f1bc6-cef9-4d93-ea50-94821b3b36ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner', 'print_info']\n",
            "After tokenization, this doc has 5 tokens.\n",
            "The part-of-speech tags are: ['PRON', 'AUX', 'DET', 'NOUN', 'PUNCT']\n",
            "This is a pretty short document.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@Language.component(\"custom_sentencizer\")\n",
        "def custom_sentencizer(doc):\n",
        "    for i, token in enumerate(doc[:-2]):\n",
        "        # Define sentence start if pipe + titlecase token\n",
        "        if token.text == \"|\" and doc[i + 1].is_title:\n",
        "            doc[i + 1].is_sent_start = True\n",
        "        else:\n",
        "            # Explicitly set sentence start to False otherwise, to tell\n",
        "            # the parser to leave those tokens alone\n",
        "            doc[i + 1].is_sent_start = False\n",
        "    return doc\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.add_pipe(\"custom_sentencizer\", before=\"parser\")  # Insert before the parser\n",
        "doc = nlp(\"This is. A sentence. | This is. Another sentence.\")\n",
        "for sent in doc.sents:\n",
        "    print(sent.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzZTmkBnllUg",
        "outputId": "0d364e4c-6c15-4c5b-cb2d-dab2ba953fa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is. A sentence. |\n",
            "This is. Another sentence.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.pipeline import EntityRuler\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "animal = [\"cat\", \"dog\", \"artic fox\"]\n",
        "#ruler = EntityRuler(nlp)\n",
        "ruler = nlp.add_pipe(\"entity_ruler\")\n",
        "for a in animal:\n",
        "    ruler.add_patterns([{\"label\": \"animal\", \"pattern\": a}])\n",
        "\n",
        "\n",
        "\n",
        "doc = nlp(\"There is no cat in the house and no artic fox in the basement\")\n",
        "\n",
        "with doc.retokenize() as retokenizer:\n",
        "    for ent in doc.ents:\n",
        "        retokenizer.merge(doc[ent.start:ent.end])\n",
        "\n",
        "\n",
        "from spacy.matcher import Matcher\n",
        "matcher = Matcher(nlp.vocab)\n",
        "pattern =[[{'lower': 'no'}],[{'ENT_TYPE': {'REGEX': 'animal', 'OP': '+'}}]]\n",
        "matcher.add('no animal', pattern)\n",
        "matches = matcher(doc)\n",
        "\n",
        "\n",
        "for match_id, start, end in matches:\n",
        "    span = doc[start:end]\n",
        "    print(span)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "DlxhTEopg8vb",
        "outputId": "78ccfff9-136a-431c-a1d1-25da1ce7e30f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MatchPatternError",
          "evalue": "Invalid token patterns for matcher rule 'no animal'\n\nPattern 0:\n\n\nPattern 1:\n- [pattern -> 0 -> ENT_TYPE -> OP] extra fields not permitted\n- [pattern -> 0 -> ENT_TYPE] str type expected\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMatchPatternError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-ca393f12776d>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mmatcher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'lower'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'no'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'ENT_TYPE'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'REGEX'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'animal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'OP'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'+'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mmatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'no animal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatterns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/matcher/matcher.pyx\u001b[0m in \u001b[0;36mspacy.matcher.matcher.Matcher.add\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mMatchPatternError\u001b[0m: Invalid token patterns for matcher rule 'no animal'\n\nPattern 0:\n\n\nPattern 1:\n- [pattern -> 0 -> ENT_TYPE -> OP] extra fields not permitted\n- [pattern -> 0 -> ENT_TYPE] str type expected\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icLqVyxNawno"
      },
      "source": [
        "<font color=green>Here we see that periods no longer affect segmentation, only linebreaks do. This would be appropriate when working with a long list of tweets, for instance.</font>\n",
        "## Next Up: POS Assessment"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}