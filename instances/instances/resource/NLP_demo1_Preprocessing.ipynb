{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLP_demo1: Text preprocessing using NLTK tool"
      ],
      "metadata": {
        "id": "rZLUF1aTMZuJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "un-HeGBMHSfF"
      },
      "outputs": [],
      "source": [
        "#tokenization\n",
        "from nltk import sent_tokenize, word_tokenize\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IcPTdpHfNOn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "PCa0N8wcIOcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Preprocessing task#1 : Tokenization!"
      ],
      "metadata": {
        "id": "wsDTpeawH60L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"hi Sonal, how are you doing? I will be traveling to your city. Lets meet\"\n",
        "sent_tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdacPuEpH7qn",
        "outputId": "134f220b-c6f6-4f8e-98d6-29122b720af3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hi Sonal, how are you doing?',\n",
              " 'I will be traveling to your city.',\n",
              " 'Lets meet']"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7PG_hxOIBTn",
        "outputId": "c3e99367-f09f-4a8f-8d9e-ef83e22677c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hi',\n",
              " 'Sonal',\n",
              " ',',\n",
              " 'how',\n",
              " 'are',\n",
              " 'you',\n",
              " 'doing',\n",
              " '?',\n",
              " 'I',\n",
              " 'will',\n",
              " 'be',\n",
              " 'traveling',\n",
              " 'to',\n",
              " 'your',\n",
              " 'city',\n",
              " '.',\n",
              " 'Lets',\n",
              " 'meet']"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Preprocessing task #1 : Tokenization with N-grams"
      ],
      "metadata": {
        "id": "-gAUwOMepSwE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ngrams"
      ],
      "metadata": {
        "id": "pW-F_ps_LUZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"I like to drink tea in winter season\""
      ],
      "metadata": {
        "id": "1uw6jVzVLn65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n=3\n",
        "ngrams(word_tokenize(input_text), n)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uW7uh2j_LzDx",
        "outputId": "c447bbf3-1bca-464f-de2d-f29c4c7ebdce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<zip at 0x7bce6a357000>"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for gram in ngrams(word_tokenize(input_text), n):\n",
        "  print(gram)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cNJxVVgMKA3",
        "outputId": "a9f286eb-fffc-4a75-b627-b9dfe0e8bafc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('I', 'like', 'to')\n",
            "('like', 'to', 'drink')\n",
            "('to', 'drink', 'tea')\n",
            "('drink', 'tea', 'in')\n",
            "('tea', 'in', 'winter')\n",
            "('in', 'winter', 'season')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Preprocessing task #1 : Tokenization with Regular Expressions"
      ],
      "metadata": {
        "id": "PhbVFyxwpclE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \".That U.S.A. poster-print costs $12.40..\"\n"
      ],
      "metadata": {
        "id": "U7ow-9RtloQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import RegexpTokenizer() method from nltk\n",
        "from nltk.tokenize import RegexpTokenizer"
      ],
      "metadata": {
        "id": "osqRgp3llrMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a reference variable for Class RegexpTokenizer\n",
        "#This regexp used to match delimiters\n",
        "tk = RegexpTokenizer('\\s+', gaps = False)"
      ],
      "metadata": {
        "id": "7ZlH9dvRlvUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a string input\n",
        "gfg = \"That New York poster-print costs $12.40..\""
      ],
      "metadata": {
        "id": "pJJPUqwKpsXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use tokenize method\n",
        "geek = tk.tokenize(gfg)\n",
        "\n",
        "print(geek)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_nAY6XUpvMs",
        "outputId": "85aa0c3a-0883-4d4e-c177-aa647219c285"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' ', ' ', ' ', ' ', ' ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = \"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.\""
      ],
      "metadata": {
        "id": "uT5pjSPnxWHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RegexpTokenizer(r'\\w+|\\$[\\d\\.]+|\\S+')"
      ],
      "metadata": {
        "id": "l2ncaQqrxZRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.tokenize(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajvh4Ifzxp4X",
        "outputId": "fb1196fc-f265-41d2-c6ea-5a3dc6c7e167"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Good',\n",
              " 'muffins',\n",
              " 'cost',\n",
              " '$3.88',\n",
              " 'in',\n",
              " 'New',\n",
              " 'York',\n",
              " '.',\n",
              " 'Please',\n",
              " 'buy',\n",
              " 'me',\n",
              " 'two',\n",
              " 'of',\n",
              " 'them',\n",
              " '.',\n",
              " 'Thanks',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "capword_tokenizer = RegexpTokenizer(r'[A-Z]\\w+')"
      ],
      "metadata": {
        "id": "6hpNMycgywB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "capword_tokenizer.tokenize(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wi0vqKZVyyh_",
        "outputId": "3fdbf773-09a7-41e0-a62b-54d060d8a6f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Good', 'New', 'York', 'Please', 'Thanks']"
            ]
          },
          "metadata": {},
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# #several subclasses of RegexpTokenizer that use pre-defined regular expressions"
      ],
      "metadata": {
        "id": "ncVNglKAy8Jz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import BlanklineTokenizer\n",
        "# Uses '\\s*\\n\\s*\\n\\s*'"
      ],
      "metadata": {
        "id": "ts1Y0lyfy_YX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = \"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.\"\n",
        "#s = \"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.\""
      ],
      "metadata": {
        "id": "AO92bgLazg4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BlanklineTokenizer().tokenize(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FT9-SFBAzERH",
        "outputId": "49e0769c-5423-477b-d34a-326c78369536"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.',\n",
              " 'Thanks.']"
            ]
          },
          "metadata": {},
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# All of the regular expression tokenizers are also available as functions"
      ],
      "metadata": {
        "id": "zYsri0w-Ahmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import regexp_tokenize, blankline_tokenize"
      ],
      "metadata": {
        "id": "NsYwVA_uAhB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regexp_tokenize(s, pattern=r'\\w+|\\$[\\d\\.]+|\\S+')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYy9ToZhAiMf",
        "outputId": "8d71fbd1-7b6b-4e1c-f385-676852a97f66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Good',\n",
              " 'muffins',\n",
              " 'cost',\n",
              " '$3.88',\n",
              " 'in',\n",
              " 'New',\n",
              " 'York',\n",
              " '.',\n",
              " 'Please',\n",
              " 'buy',\n",
              " 'me',\n",
              " 'two',\n",
              " 'of',\n",
              " 'them',\n",
              " '.',\n",
              " 'Thanks',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "blankline_tokenize(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODR-XtRRAlvf",
        "outputId": "0b273f2f-37c4-44a6-fcbe-1d6081729640"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.',\n",
              " 'Thanks.']"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Whitespace Tokenizer - Unigram"
      ],
      "metadata": {
        "id": "dPTxsRGJqWiU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import WhitespaceTokenizer() method from nltk\n",
        "from nltk.tokenize import WhitespaceTokenizer\n"
      ],
      "metadata": {
        "id": "bR40izEWqMqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a reference variable for Class WhitespaceTokenizer\n",
        "tk = WhitespaceTokenizer()\n"
      ],
      "metadata": {
        "id": "huAIQJ1VqM1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Create a string input\n",
        "gfg = \"GeeksforGeeks \\nis\\t for geeks\"\n",
        "gfg1 = \"The price\\t of burger \\nin BurgerKing is Rs.36.\\n\""
      ],
      "metadata": {
        "id": "1Lnb1fjVr7vE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use tokenize method\n",
        "geek = tk.tokenize(gfg)\n",
        "\n",
        "print(geek)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boqf9wp9qM5L",
        "outputId": "fa8051b1-62bd-44f9-9919-bf8d8772fff1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['GeeksforGeeks', 'is', 'for', 'geeks']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "geek = tk.tokenize(gfg1)\n",
        "\n",
        "print(geek)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_hl0OUMrz48",
        "outputId": "40d2be95-5c7a-41a6-f4b5-4cdfafd7a2d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'price', 'of', 'burger', 'in', 'BurgerKing', 'is', 'Rs.36.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Punctuation Tokenizer - Tokenize a text into a sequence of alphabetic and non-alphabetic characters, ***using the regexp \\w+|[^\\w\\s]+***"
      ],
      "metadata": {
        "id": "YxWqYrxEsPR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import WordPunctTokenizer() method from nltk\n",
        "from nltk.tokenize import WordPunctTokenizer"
      ],
      "metadata": {
        "id": "u-tX-h2bsNuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a reference variable for Class WordPunctTokenizer\n",
        "tk = WordPunctTokenizer()"
      ],
      "metadata": {
        "id": "8aHMz_qHsjUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a string input\n",
        "gfg = \"GeeksforGeeks...$$&* \\nis\\t for geeks\""
      ],
      "metadata": {
        "id": "ylDjecmAsjYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use tokenize method\n",
        "geek = tk.tokenize(gfg)\n",
        "\n",
        "print(geek)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Q1GF6qPsoAs",
        "outputId": "f6c6b6be-1b87-450c-92aa-78aabe62a13f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['GeeksforGeeks', '...$$&*', 'is', 'for', 'geeks']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gfg1 = \"The price\\t of burger \\nin BurgerKing is Rs.36.\\n\""
      ],
      "metadata": {
        "id": "bB1_XtGpsv8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use tokenize method\n",
        "geek = tk.tokenize(gfg1)\n",
        "\n",
        "print(geek)"
      ],
      "metadata": {
        "id": "iPQhM4g0s2N8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Source code for nltk.tokenize.regexp**\n",
        "https://www.nltk.org/_modules/nltk/tokenize/regexp.html"
      ],
      "metadata": {
        "id": "Zs1iMcPvCQ10"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oluNy56sDMTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# nltk.tokenize.TabTokenizer()\n",
        "# Tab Tokenizer - to extract tokens on the basis of tabs"
      ],
      "metadata": {
        "id": "SyTeYcN5DQ4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import TabTokenizer() method from nltk\n",
        "from nltk.tokenize import TabTokenizer"
      ],
      "metadata": {
        "id": "qO4qL3r2DMV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a reference variable for Class TabTokenizer\n",
        "tk = TabTokenizer()"
      ],
      "metadata": {
        "id": "3yNBFnNNDk42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a string input\n",
        "gfg = \"Geeksfor\\tGeeks..\\t.$$&* \\nis\\t for geeks\""
      ],
      "metadata": {
        "id": "BQ3dFRmvDpBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use tokenize method\n",
        "geek = tk.tokenize(gfg)\n",
        "\n",
        "print(geek)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wujA-tJoDrxs",
        "outputId": "493880ba-6b0d-4017-cbe4-1958d233cbd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Geeksfor', 'Geeks..', '.$$&* \\nis', ' for geeks']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# nltk.tokenize.SpaceTokenizer()\n",
        "# Space Tokenizer - to extract tokens on the basis of spaces"
      ],
      "metadata": {
        "id": "sZVImSSDEGMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import SpaceTokenizer() method from nltk\n",
        "from nltk.tokenize import SpaceTokenizer"
      ],
      "metadata": {
        "id": "gtp7CwOJEMVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a reference variable for Class SpaceTokenizer\n",
        "tk = SpaceTokenizer()\n"
      ],
      "metadata": {
        "id": "DBvGGO2ZEQVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a string input\n",
        "gfg = \"Geeksfor Geeks.. .$$&* \\nis\\t for geeks\"\n",
        "\n",
        "# Use tokenize method\n",
        "geek = tk.tokenize(gfg)\n",
        "\n",
        "print(geek)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIRi3yCMERmm",
        "outputId": "682f438f-993e-4438-e02a-4cb6392e821c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Geeksfor', 'Geeks..', '.$$&*', '\\nis\\t', 'for', 'geeks']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# nltk.tokenize.LineTokenizer()\n",
        "# Line Tokenizer - to extract tokens on the basis of newlines"
      ],
      "metadata": {
        "id": "kmLYK_m9FCbm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import LineTokenizer() method from nltk\n",
        "from nltk.tokenize import LineTokenizer"
      ],
      "metadata": {
        "id": "tH6X1fjkEhfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a reference variable for Class LineTokenizer\n",
        "tk = LineTokenizer()"
      ],
      "metadata": {
        "id": "q85eA5QAFKv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a string input\n",
        "gfg = \"GeeksforGeeks...$$&* \\nis\\n for geeks\"\n",
        "\n",
        "# Use tokenize method\n",
        "geek = tk.tokenize(gfg)\n",
        "\n",
        "print(geek)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jxHlLVpFKzk",
        "outputId": "92e13bfc-867c-4b86-8cf9-d3da4f203b1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['GeeksforGeeks...$$&* ', 'is', ' for geeks']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# nltk.tokenize.ConditionalFreqDist()\n",
        "# Tokenizer - to count the frequency of words in a sentence and return a dictionary"
      ],
      "metadata": {
        "id": "DaXABO4vFwal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import ConditionalFreqDist() method from nltk\n",
        "from nltk.probability import ConditionalFreqDist\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "o8yJ04AVF_Pw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a reference variable for Class\n",
        "tk = ConditionalFreqDist()"
      ],
      "metadata": {
        "id": "unoB6Du-GBgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a string input\n",
        "gfg = \"Geeks for Heeks\"\n",
        "\n",
        "for word in word_tokenize(gfg):\n",
        "   condition = len(word)\n",
        "   print(word)\n",
        "   print(condition)\n",
        "   tk[condition][word] += 1\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cwGaySIGDXu",
        "outputId": "939d3995-1f42-4e40-aab2-a7ed84f6f86b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Geeks\n",
            "5\n",
            "for\n",
            "3\n",
            "Heeks\n",
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tk[5][\"Heeks\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNSgmHjyGJOR",
        "outputId": "6bb136f1-4fa6-4ce9-8dd9-7b40a0318654"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# nltk.tokenize.SExprTokenizer()\n",
        "# Tokenizer - to look for proper brackets to make tokens."
      ],
      "metadata": {
        "id": "j4r6qDN1HRN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import SExprTokenizer() method from nltk\n",
        "from nltk.tokenize import SExprTokenizer"
      ],
      "metadata": {
        "id": "wwbzr7pgHcBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a reference variable for Class SExprTokenizer\n",
        "tk = SExprTokenizer()"
      ],
      "metadata": {
        "id": "WWhYXFgYHdte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a string input\n",
        "#gfg = \"{ a * { b + c }}ab{ a-c }\"\n",
        "gfg = \"( a * ( b + c ))ab( a-c )\"\n",
        "\n",
        "# Use tokenize method\n",
        "geek = tk.tokenize(gfg)\n",
        "\n",
        "print(geek)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGTodgr3HdwX",
        "outputId": "3d3f87ad-8d4a-4dd7-aefc-7199a6b5903a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['( a * ( b + c ))', 'ab', '( a-c )']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a Basic hardcoded ChatBot using Python-NLTK"
      ],
      "metadata": {
        "id": "IDFbm-odJT9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.chat.util import Chat, reflections\n",
        "\n",
        "pairs =[\n",
        "    ['my name is (.*)', ['Hello %1! ']],\n",
        "    ['(hi|hello|hey|holla|hola)', ['Hey there !', 'Hi there !', 'Hey !']],\n",
        "    ['(.*) your name ?', ['My name is Geeky']],\n",
        "    ['(.*) do you do ?', ['We provide a platform for tech enthusiasts, a wide range of options !']],\n",
        "    ['(.*) created you ?', ['Geeksforgeeks created me using python and NLTK']]\n",
        "]\n",
        "\n",
        "chat = Chat(pairs, reflections)\n",
        "chat.converse()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PT8WMbEgIMvU",
        "outputId": "cbb7df19-9486-4e6c-d879-e469a7c8452a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">hi\n",
            "Hey there !\n",
            ">how do you do\n",
            "We provide a platform for tech enthusiasts, a wide range of options !\n",
            ">how d y do\n",
            "None\n",
            ">\u0004\n",
            "quit\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KcYo65mWVpFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Preprocessing task#2 : Stemming"
      ],
      "metadata": {
        "id": "dnBN_Ar_owOG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In NLTK, stemmerI, which have stem() method, interface has all the stemmers like PorterStemmer, SnowballStemm, RegExpStemme, LancasterStem"
      ],
      "metadata": {
        "id": "IDg6HoazXCEz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# #Porter Stemming Algorithm\n",
        "most common stemming algorithms, designed to remove and replace well-known suffixes of English words"
      ],
      "metadata": {
        "id": "9UDtQxwXXbII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#stemming\n",
        "from nltk.stem import  PorterStemmer"
      ],
      "metadata": {
        "id": "KA5W2gx8IBXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n"
      ],
      "metadata": {
        "id": "nk3f9z1FI7kJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stemmer.stem(\"controlling\"))\n",
        "print(stemmer.stem(\"playing\"))\n",
        "print(stemmer.stem(\"plays\"))\n",
        "print(stemmer.stem(\"played\"))"
      ],
      "metadata": {
        "id": "NHb9P7OcJEL4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "299851c9-041c-42fa-fae4-4904c65cb53c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "control\n",
            "play\n",
            "play\n",
            "play\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(stemmer.stem(\"increases\"))"
      ],
      "metadata": {
        "id": "M6faIjTUJWp_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3947989-c1aa-46e6-915f-a54ce166520f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "increas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wR9gcte4WJqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# #Lancaster stemming algorithm\n",
        "It was developed at Lancaster University"
      ],
      "metadata": {
        "id": "_ZTxgOAeXvOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer"
      ],
      "metadata": {
        "id": "6zA-EiX6Xxva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Lanc_stemmer = LancasterStemmer()"
      ],
      "metadata": {
        "id": "PwvAVDb8XxyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Lanc_stemmer.stem('increases')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "sWljSrbtXx1J",
        "outputId": "e867cce5-25c7-4836-9e35-2d73672fd648"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'increas'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 191
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regular Expression stemming algorithm"
      ],
      "metadata": {
        "id": "e3tH70BNX-X7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "basically takes a single regular expression and removes any prefix or suffix that matches the expression.\n",
        "It helps to create our own stemmer."
      ],
      "metadata": {
        "id": "lDhNuGgnYHUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer"
      ],
      "metadata": {
        "id": "EZoQ2O2fYCJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Reg_stemmer = RegexpStemmer(\"ing\")"
      ],
      "metadata": {
        "id": "eNIjwxKGYCL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Reg_stemmer.stem('ingeating')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mRFOcAc9YWs6",
        "outputId": "8e45daf8-f45a-40cf-bcea-dbb6948d5d53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 195
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Snowball stemming algorithm!"
      ],
      "metadata": {
        "id": "2Xba_KyOYs_h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It supports 15 non-English languages"
      ],
      "metadata": {
        "id": "0075Ik8jYw1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer"
      ],
      "metadata": {
        "id": "3mP8gbBvYwPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SnowballStemmer.languages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B98YnHzkY35j",
        "outputId": "ea733cd1-8076-42e1-c34e-30bbe67849e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('arabic',\n",
              " 'danish',\n",
              " 'dutch',\n",
              " 'english',\n",
              " 'finnish',\n",
              " 'french',\n",
              " 'german',\n",
              " 'hungarian',\n",
              " 'italian',\n",
              " 'norwegian',\n",
              " 'porter',\n",
              " 'portuguese',\n",
              " 'romanian',\n",
              " 'russian',\n",
              " 'spanish',\n",
              " 'swedish')"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "French_stemmer = SnowballStemmer('french')"
      ],
      "metadata": {
        "id": "64soAMEEY38S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "French_stemmer.stem ('Bonjoura')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HrQilTl5Y_xQ",
        "outputId": "536328de-e05c-4cc9-9963-134ec231bb3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'bonjour'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 198
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Preprocessing task #3 : Lemmatization"
      ],
      "metadata": {
        "id": "RG0maUPQo5Zs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK provides WordNetLemmatizer class which is a thin wrapper around the wordnet corpus. This class uses morphy() function to the WordNet CorpusReader class to find a lemma."
      ],
      "metadata": {
        "id": "sc8MeyhlZc6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#lemmatization\n",
        "from nltk.stem import  WordNetLemmatizer"
      ],
      "metadata": {
        "id": "dXEJo5raJcQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemm = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "AWyze_YVJjS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "zrSrpTfuJ8Xp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdcef3cc-272a-4842-c259-bae56e6a5d4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(lemm.lemmatize(\"increases\"))"
      ],
      "metadata": {
        "id": "VjgJ6zMVJlq4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de092c92-49a4-4aed-9054-2ef60dc7861f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "increase\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(lemm.lemmatize(\"running\"))"
      ],
      "metadata": {
        "id": "jzUoDRXtKHbg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28e234be-a53c-4193-c108-714155b81b60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(lemm.lemmatize(\"running\", pos='v'))"
      ],
      "metadata": {
        "id": "8vL6uz4kKNq7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19b59f0e-58fc-442e-9e58-24dea575bdef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TextAttack library and Data Augmentation#\n",
        "a Python framework created especially for use in data augmentation.\n",
        "\n",
        "The six types of augmenters are:\n",
        "\n",
        "* WordNetAugmenter\n",
        "* EmbeddingAugmenter\n",
        "* CharSwapAugmenter\n",
        "* CheckListAugmenter\n",
        "* EasyDataAugmenter\n",
        "* CLAREAugumenter"
      ],
      "metadata": {
        "id": "Q-v2IzmGdqwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textattack"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FYaWj77dyVK",
        "outputId": "f0f09a06-c65e-4edd-c42d-a8543cc94b85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textattack\n",
            "  Downloading textattack-0.3.9-py3-none-any.whl (436 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.8/436.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bert-score>=0.3.5 (from textattack)\n",
            "  Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: editdistance in /usr/local/lib/python3.10/dist-packages (from textattack) (0.6.2)\n",
            "Collecting flair (from textattack)\n",
            "  Downloading flair-0.13.1-py3-none-any.whl (388 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.3/388.3 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from textattack) (3.13.1)\n",
            "Collecting language-tool-python (from textattack)\n",
            "  Downloading language_tool_python-2.7.1-py3-none-any.whl (34 kB)\n",
            "Collecting lemminflect (from textattack)\n",
            "  Downloading lemminflect-0.2.3-py3-none-any.whl (769 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m769.7/769.7 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lru-dict (from textattack)\n",
            "  Downloading lru_dict-1.3.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Collecting datasets>=2.4.0 (from textattack)\n",
            "  Downloading datasets-2.16.1-py3-none-any.whl (507 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from textattack) (3.8.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from textattack) (1.23.5)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from textattack) (1.5.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from textattack) (1.11.4)\n",
            "Requirement already satisfied: torch!=1.8,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from textattack) (2.1.0+cu121)\n",
            "Requirement already satisfied: transformers>=4.30.0 in /usr/local/lib/python3.10/dist-packages (from textattack) (4.35.2)\n",
            "Collecting terminaltables (from textattack)\n",
            "  Downloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from textattack) (4.66.1)\n",
            "Collecting word2number (from textattack)\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting num2words (from textattack)\n",
            "  Downloading num2words-0.5.13-py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.3/143.3 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from textattack) (10.1.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from textattack) (1.7.1)\n",
            "Collecting pinyin>=0.4.0 (from textattack)\n",
            "  Downloading pinyin-0.4.0.tar.gz (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from textattack) (0.42.1)\n",
            "Collecting OpenHowNet (from textattack)\n",
            "  Downloading OpenHowNet-2.0-py3-none-any.whl (18 kB)\n",
            "Collecting pycld2 (from textattack)\n",
            "  Downloading pycld2-0.41.tar.gz (41.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.4/41.4 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting click<8.1.0 (from textattack)\n",
            "  Downloading click-8.0.4-py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.5/97.5 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert-score>=0.3.5->textattack) (2.31.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert-score>=0.3.5->textattack) (3.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert-score>=0.3.5->textattack) (23.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (0.6)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets>=2.4.0->textattack)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (3.4.1)\n",
            "Collecting multiprocess (from datasets>=2.4.0->textattack)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (0.20.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (6.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->textattack) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->textattack) (2023.3.post1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->textattack) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->textattack) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->textattack) (0.4.1)\n",
            "Collecting boto3>=1.20.27 (from flair->textattack)\n",
            "  Downloading boto3-1.34.15-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bpemb>=0.3.2 (from flair->textattack)\n",
            "  Downloading bpemb-0.3.4-py3-none-any.whl (19 kB)\n",
            "Collecting conllu>=4.0 (from flair->textattack)\n",
            "  Downloading conllu-4.5.3-py2.py3-none-any.whl (16 kB)\n",
            "Collecting deprecated>=1.2.13 (from flair->textattack)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting ftfy>=6.1.0 (from flair->textattack)\n",
            "  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gdown>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (4.6.6)\n",
            "Requirement already satisfied: gensim>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (4.3.2)\n",
            "Collecting janome>=0.4.2 (from flair->textattack)\n",
            "  Downloading Janome-0.5.0-py2.py3-none-any.whl (19.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langdetect>=1.0.9 (from flair->textattack)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (4.9.4)\n",
            "Collecting mpld3>=0.3 (from flair->textattack)\n",
            "  Downloading mpld3-0.5.10-py3-none-any.whl (202 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.6/202.6 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pptree>=3.1 (from flair->textattack)\n",
            "  Downloading pptree-3.1.tar.gz (3.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytorch-revgrad>=0.2.0 (from flair->textattack)\n",
            "  Downloading pytorch_revgrad-0.2.0-py3-none-any.whl (4.6 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (1.2.2)\n",
            "Collecting segtok>=1.5.11 (from flair->textattack)\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Collecting sqlitedict>=2.0.0 (from flair->textattack)\n",
            "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tabulate>=0.8.10 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (0.9.0)\n",
            "Collecting transformer-smaller-training-vocab>=0.2.3 (from flair->textattack)\n",
            "  Downloading transformer_smaller_training_vocab-0.3.3-py3-none-any.whl (14 kB)\n",
            "Collecting urllib3<2.0.0,>=1.0.0 (from flair->textattack)\n",
            "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wikipedia-api>=0.5.7 (from flair->textattack)\n",
            "  Downloading Wikipedia_API-0.6.0-py3-none-any.whl (14 kB)\n",
            "Collecting semver<4.0.0,>=3.0.0 (from flair->textattack)\n",
            "  Downloading semver-3.0.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->textattack) (1.3.2)\n",
            "Collecting docopt>=0.6.2 (from num2words->textattack)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting anytree (from OpenHowNet->textattack)\n",
            "  Downloading anytree-2.12.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.9/44.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from OpenHowNet->textattack) (67.7.2)\n",
            "Collecting botocore<1.35.0,>=1.34.15 (from boto3>=1.20.27->flair->textattack)\n",
            "  Downloading botocore-1.34.15-py3-none-any.whl (11.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.20.27->flair->textattack)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3>=1.20.27->flair->textattack)\n",
            "  Downloading s3transfer-0.10.0-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece (from bpemb>=0.3.2->flair->textattack)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.13->flair->textattack) (1.14.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (4.0.3)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy>=6.1.0->flair->textattack) (0.2.12)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown>=4.4.0->flair->textattack) (1.16.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.4.0->flair->textattack) (4.11.2)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim>=4.2.0->flair->textattack) (6.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (4.47.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (3.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score>=0.3.5->textattack) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score>=0.3.5->textattack) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score>=0.3.5->textattack) (2023.11.17)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->flair->textattack) (3.2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->textattack) (3.20.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.8,>=1.7.0->textattack) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.8,>=1.7.0->textattack) (1.3.0)\n",
            "Collecting accelerate>=0.20.3 (from transformers>=4.30.0->textattack)\n",
            "  Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.4.0->flair->textattack) (2.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers>=4.30.0->textattack) (5.9.5)\n",
            "Building wheels for collected packages: pinyin, pycld2, word2number, docopt, langdetect, pptree, sqlitedict\n",
            "  Building wheel for pinyin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pinyin: filename=pinyin-0.4.0-py3-none-any.whl size=3630476 sha256=880f1df35f2b6999d6440eaa8dc829741abff1aa457ee03e8e73a72e19aa4c7a\n",
            "  Stored in directory: /root/.cache/pip/wheels/33/38/af/616fc6f154aa5bae65a1da12b22d79943434269f0468ff9b3f\n",
            "  Building wheel for pycld2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycld2: filename=pycld2-0.41-cp310-cp310-linux_x86_64.whl size=9904026 sha256=331b1c2c1bc84ab7abbc5eac78b26ba8da349e89750505545bf3bbf9d51496e8\n",
            "  Stored in directory: /root/.cache/pip/wheels/be/81/31/240c89c845e008a93d98542325270007de595bfd356eb0b06c\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5568 sha256=204eda23ba056ed7cb6ddc725500fc8ecef4a841e48197d470627d841b11ed3d\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/ff/26/d3cfbd971e96c5aa3737ecfced81628830d7359b55fbb8ca3b\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=84554cf3fc41b4f36a523f21e163aa52afae54f941a87e53cf79862ae98add6e\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=81d5123c19a2e1703f6d81396b5ba232fbb6769fca5885551a73b60a40f2f044\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "  Building wheel for pptree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4608 sha256=38aa0585c4c45893715455a4ce567719888c4ac58d4d800fe79c0e955c0eb158\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/b6/0e/6f26eb9e6eb53ff2107a7888d72b5a6a597593956113037828\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16863 sha256=dac94b3a81585a31a04f9cd90c284d3a80d61abb114dbd4de9a0dd13f7fbcf4f\n",
            "  Stored in directory: /root/.cache/pip/wheels/79/d6/e7/304e0e6cb2221022c26d8161f7c23cd4f259a9e41e8bbcfabd\n",
            "Successfully built pinyin pycld2 word2number docopt langdetect pptree sqlitedict\n",
            "Installing collected packages: word2number, sqlitedict, sentencepiece, pycld2, pptree, pinyin, janome, docopt, urllib3, terminaltables, semver, segtok, num2words, lru-dict, lemminflect, langdetect, jmespath, ftfy, dill, deprecated, conllu, click, anytree, multiprocess, botocore, wikipedia-api, s3transfer, pytorch-revgrad, OpenHowNet, mpld3, language-tool-python, bpemb, datasets, boto3, accelerate, bert-score, transformer-smaller-training-vocab, flair, textattack\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.1.7\n",
            "    Uninstalling click-8.1.7:\n",
            "      Successfully uninstalled click-8.1.7\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed OpenHowNet-2.0 accelerate-0.25.0 anytree-2.12.1 bert-score-0.3.13 boto3-1.34.15 botocore-1.34.15 bpemb-0.3.4 click-8.0.4 conllu-4.5.3 datasets-2.16.1 deprecated-1.2.14 dill-0.3.7 docopt-0.6.2 flair-0.13.1 ftfy-6.1.3 janome-0.5.0 jmespath-1.0.1 langdetect-1.0.9 language-tool-python-2.7.1 lemminflect-0.2.3 lru-dict-1.3.0 mpld3-0.5.10 multiprocess-0.70.15 num2words-0.5.13 pinyin-0.4.0 pptree-3.1 pycld2-0.41 pytorch-revgrad-0.2.0 s3transfer-0.10.0 segtok-1.5.11 semver-3.0.2 sentencepiece-0.1.99 sqlitedict-2.1.0 terminaltables-3.1.10 textattack-0.3.9 transformer-smaller-training-vocab-0.3.3 urllib3-1.26.18 wikipedia-api-0.6.0 word2number-1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WordnetAugmenter"
      ],
      "metadata": {
        "id": "kJnS27HweQlo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textattack.augmentation import WordNetAugmenter\n",
        "\n",
        "augmenter = WordNetAugmenter()\n",
        "\n",
        "# Example usage:\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "augmented_sentence = augmenter.augment(sentence)\n",
        "\n",
        "print(f\"Original Sentence: {sentence}\")\n",
        "print(f\"Augmented Sentence: {augmented_sentence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cz10wjGYeL1f",
        "outputId": "376f84ce-e265-41f9-9baf-58e77038e75f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "textattack: Updating TextAttack package dependencies.\n",
            "textattack: Downloading NLTK required packages.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package omw to /root/nltk_data...\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence: The quick brown fox jumps over the lazy dog.\n",
            "Augmented Sentence: ['The quick brown fox jumps over the lazy click.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding Augmenter\n",
        "\n",
        "The EmbeddingAugmenter in TextAttack is designed to perform data augmentation by substituting words in a sentence with their word embeddings. This augments the text while preserving semantic meaning."
      ],
      "metadata": {
        "id": "jlJ0s1GGeYbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textattack.augmentation import EmbeddingAugmenter\n",
        "\n",
        "# Initialize the EmbeddingAugmenter\n",
        "embed_aug = EmbeddingAugmenter()\n",
        "\n",
        "# Example usage:\n",
        "original_text = \"TextAttack is a powerful library for NLP.\"\n",
        "augmented_text = embed_aug.augment(original_text)\n",
        "\n",
        "print(f\"Original Text: {original_text}\")\n",
        "print(f\"Augmented Text: {augmented_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADBSy1yidyX0",
        "outputId": "d8d5fe9e-d193-4628-ac27-aaa1e4b7f39f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "textattack: Downloading https://textattack.s3.amazonaws.com/word_embeddings/paragramcf.\n",
            "100%|██████████| 481M/481M [00:12<00:00, 37.6MB/s]\n",
            "textattack: Unzipping file /root/.cache/textattack/tmpe9ivcpdd.zip to /root/.cache/textattack/word_embeddings/paragramcf.\n",
            "textattack: Successfully saved word_embeddings/paragramcf to cache.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: TextAttack is a powerful library for NLP.\n",
            "Augmented Text: ['TextAttack is a powerful bookcase for NLP.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CharSwapAugmenter\n",
        "The CharSwapAugmenter in TextAttack is an augmentation technique that randomly swaps adjacent characters in a word to introduce small, character-level perturbations."
      ],
      "metadata": {
        "id": "yF6PYU3Wek5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textattack.augmentation import CharSwapAugmenter\n",
        "\n",
        "# Initialize the CharSwapAugmenter\n",
        "char_swap_augmenter = CharSwapAugmenter()\n",
        "\n",
        "# Example usage:\n",
        "original_text = \"TextAttack is a powerful library for NLP.\"\n",
        "augmented_text = char_swap_augmenter.augment(original_text)\n",
        "\n",
        "print(f\"Original Text: {original_text}\")\n",
        "print(f\"Augmented Text: {augmented_text}\")"
      ],
      "metadata": {
        "id": "12VI3vQ3dyay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checklist Augmenter\n",
        "The ChecklistAugmenter in TextAttack is an augmentation technique that uses pre-defined transformations to generate perturbed versions of the input text."
      ],
      "metadata": {
        "id": "ADXSbNvuesZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textattack.augmentation import CheckListAugmenter\n",
        "\n",
        "# Sample text\n",
        "text = \"TextAttack is a powerful library for NLP.\"\n",
        "\n",
        "# Initialize the CheckListAugmenter\n",
        "checklist_augmenter = CheckListAugmenter()\n",
        "\n",
        "# Apply CheckList transformations\n",
        "augmented_text = checklist_augmenter.augment(text)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Original Text: {text}\")\n",
        "print(f\"Augmented Text: {augmented_text}\")"
      ],
      "metadata": {
        "id": "amiDj5rfezfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EasyDataAugmenter\n",
        "EDA enhances text by incorporating a mix of word insertions, substitutions, and deletions."
      ],
      "metadata": {
        "id": "YVNxj_ucfUCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textattack.augmentation import EasyDataAugmenter\n",
        "\n",
        "# Example text\n",
        "text = \"TextAttack is a powerful library for NLP.\"\n",
        "\n",
        "# Initialize the EasyDataAugmenter\n",
        "eda_augmenter = EasyDataAugmenter()\n",
        "\n",
        "# Apply EasyDataAugmenter for text augmentation\n",
        "augmented_text = eda_augmenter.augment(text)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Original Text: {text}\")\n",
        "print(f\"Augmented Text: {augmented_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_S8lMDeyeziP",
        "outputId": "62ffb34f-b0de-4901-9373-e897f0b5a42a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: TextAttack is a powerful library for NLP.\n",
            "Augmented Text: ['TextAttack is library powerful a for NLP.', 'TextAttack is a powerful library for NLP.', 'TextAttack is a mightily library for NLP.', 'TextAttack is a powerful for NLP.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Preprocessing task #4 : POS Tagging"
      ],
      "metadata": {
        "id": "-rHVxnNfo-aU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag"
      ],
      "metadata": {
        "id": "yPhq7SOLKVdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"hi Sonal, how are you doing? I will be traveling to your city. Lets meet\"\n",
        "tokens=word_tokenize(text)"
      ],
      "metadata": {
        "id": "TcD6gGJ0KeyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokens)"
      ],
      "metadata": {
        "id": "kkhf9zosKfAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "lyLbFy4JLD1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tag(tokens)"
      ],
      "metadata": {
        "id": "L2MImKAkKmxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "wordnet.synsets(\"good\")"
      ],
      "metadata": {
        "id": "8-CxNWkPLRVY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}